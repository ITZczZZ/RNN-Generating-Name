{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32097627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "['Abagail', 'Abbe', 'Abbey', 'Abbi', 'Abbie', 'Abby', 'Abigael', 'Abigail']\n"
     ]
    }
   ],
   "source": [
    "with open('./female.txt') as  f:\n",
    "    #读取txt的所有行到列表中\n",
    "    female_name_list = f.readlines()\n",
    "    print(len(female_name_list))\n",
    "    #去掉列表每一项的换行符\n",
    "male_name_list = [name.strip() for name in female_name_list]\n",
    "print(male_name_list[0:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 1, 'b': 2, 'a': 3, 'g': 4, 'i': 5, 'l': 6, '\\n': 7, 'e': 8, 'y': 9, 'r': 10, 'c': 11, 'd': 12, 'h': 13, 'n': 14, 's': 15, 'o': 16, 't': 17, 'm': 18, 'j': 19, 'x': 20, 'f': 21, 'u': 22, 'z': 23, 'p': 24, 'v': 25, 'k': 26, 'q': 27, '-': 28, 'M': 29, 'D': 30, 'C': 31, 'B': 32, 'J': 33, 'w': 34, ' ': 35, 'E': 36, \"'\": 37, 'L': 38, 'F': 39, 'G': 40, 'H': 41, 'I': 42, 'K': 43, 'N': 44, 'O': 45, 'P': 46, 'Q': 47, 'R': 48, 'S': 49, 'T': 50, 'U': 51, 'V': 52, 'W': 53, 'X': 54, 'Y': 55, 'Z': 56, 'pad': 0, 'start': 57, 'end': 58}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import json\n",
    "letter = Counter()\n",
    "for name in female_name_list:\n",
    "    for char in name:\n",
    "        letter.update(char)\n",
    "letter2num = {k:i+1 for i , k in enumerate(letter)}\n",
    "letter2num['pad'] = 0\n",
    "letter2num['start']=len(letter2num)\n",
    "letter2num['end']=len(letter2num)\n",
    "print(letter2num)\n",
    "with open('./female_letter_encode.json','w') as f:\n",
    "    json.dump(letter2num,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对数据做处理：\n",
    "1.从txt文件中读入所有名字，并以列表存储，列表的每个元素都是一个字符串\n",
    "2.每个字符串都由字母letter组成，统计所有出现过的字母\n",
    "3.在本任务中，RNN网络每一次输入应该是一个字母，因此要对字母编码\n",
    "4.编码的方式非常简单，按照顺序给每个字母配一个int,还要再加上起始符start、终止符end\n",
    "5.将所有字符串形式的名字全部编码成数组形式，并在每个名字加上起始和结束标识的编码，例如“Adam”->[0,1,10,2,3,54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[54, 1, 2, 5, 6, 7, 55], [54, 1, 8, 8, 9, 10, 55]]\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import json\n",
    "letter = Counter()\n",
    "for name in female_name_list:\n",
    "    for char in name:\n",
    "        letter.update(char)\n",
    "'''\n",
    "letter2num = {k:i+1 for i , k in enumerate(letter)}\n",
    "letter2num['pad'] = 0\n",
    "letter2num['start']=len(letter2num)\n",
    "letter2num['end']=len(letter2num)\n",
    "num2letter = {letter2num[k]:k for k in letter2num.keys()}\n",
    "print(letter2num)\n",
    "'''\n",
    "with open('./letter2num_encode.json','r') as f:\n",
    "    letter2num=json.load(f)\n",
    "encode_names=[]\n",
    "names_len = []\n",
    "for i,name in enumerate(male_name_list):\n",
    "    encode_name = [letter2num['start']] + [letter2num[c] for c in name] + [letter2num['end']]\n",
    "    encode_names.append(encode_name)\n",
    "    names_len.append(len(encode_name))\n",
    "print(encode_names[1:3])\n",
    "print(names_len.index(min(names_len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在网络中，每个名字要是一维张量，shape=(17,)，第一个元素一定是54，表示开始，中间会有55，表示结束，55后面全是0，用来占位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch\n",
    "class NamesDataset(Dataset):\n",
    "    def __init__(self,Names:list,max_len=15):   #max_len要求每个名字最多10个字母，不足10个字母在后面补pad，也就是0\n",
    "        self.Names=Names\n",
    "        #self.transform=transform\n",
    "        self.length=len(Names)\n",
    "        self.max_len=max_len\n",
    "    def __getitem__(self, index):\n",
    "        name_len = len(self.Names[index])\n",
    "        return torch.LongTensor(self.Names[index]+[0]*(self.max_len+2-name_len)),name_len    #加2是考虑有起始和终止符\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "train_data = NamesDataset(encode_names)\n",
    "train_loader = DataLoader(train_data,batch_size=20, shuffle=True,pin_memory=True)\n",
    "#print:tensor([[54, 47, 18, 21,  6, 55,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "        #      [54, 50,  8, 14, 10,  2, 12, 55,  0,  0,  0,  0,  0,  0,  0,  0,  0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN的输入虽然说是一个序列，但其实每次真正拿进去网络和隐层h一起搞的只有一个token，在这里就是一个字母\n",
    "目前的一个名字的表示已经转换为了一个shape=(12,)的一维张量，但每个字母只是一个int而已，这样当然不行\n",
    "需要把int转换成tensor\n",
    "考虑所有的字母，包括起始符、终止符和pad，一共有56个字母，即len(letter2num)=56\n",
    "可以使用nn.embedding(56,100),让56个字母，每个都用一个shape=(100,)的一维张量表示\n",
    "原本的names=(batch,max_len+2)经过embed作为网络的input=(batch,max_len+2,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class GeneratorName(nn.Module):\n",
    "    def __init__(self,letter2num:dict or None=None,letter_dim:int=100,dropout:float=0.3,hidden_size:float=100) -> None:\n",
    "        super(GeneratorName,self).__init__()\n",
    "        if(letter2num is not None):\n",
    "            self.letter2num=letter2num\n",
    "            self.letter_count=len(letter2num)\n",
    "        self.hidden_size=hidden_size    #隐状态的维度\n",
    "        self.letter_dim=letter_dim      #每个字母的维度\n",
    "        self.dropout=dropout\n",
    "        #hidden最开始是56维的one-hot向量，将其经过线性层\n",
    "\n",
    "    def load_letter2num(self,letter2num:dict):\n",
    "        self.letter2num=letter2num\n",
    "        self.letter_count=len(letter2num)\n",
    "\n",
    "    def build_layers(self):\n",
    "        self.init_hidden=nn.Sequential(\n",
    "            nn.Softmax(dim=1),\n",
    "            nn.Linear(self.letter_count,self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size,self.hidden_size),\n",
    "        )\n",
    "        self.embed_layer=nn.Embedding(self.letter_count,self.letter_dim)\n",
    "        self.rnn=nn.GRU(self.letter_dim,self.hidden_size,1)      #rnn的输入是100，因为每个字母转换成了100维向量,输出是100维向量\n",
    "        self.dropout=nn.Dropout(self.dropout)\n",
    "        self.fc=nn.Linear(self.hidden_size,self.letter_count)   #用线性层将100维变成56维\n",
    "\n",
    "    def init_hidden_state(self, names, names_len):\n",
    "        #names=(batch,max_len+2),names_len=(batch,)\n",
    "        batch_size, names_dim = names.size(0), names.size(1)\n",
    "\n",
    "        # 按照names的长短排序\n",
    "        sorted_names_lens, sorted_nams_indices = torch.sort(names_len, 0, True)\n",
    "        sorted_names = names[sorted_nams_indices]\n",
    "\n",
    "        #hidden_state = torch.zeros(batch_size,self.letter_count).to(sorted_names.device)    #隐层状态的shape=(1,batch,100)\n",
    "        hidden_state = torch.eye(self.letter_count).to(sorted_names.device)\n",
    "        first_letters=sorted_names[:,1]\n",
    "        \"\"\"\n",
    "        for i in range(batch_size):\n",
    "            hidden_state[i , first_letters[i]] = 1\n",
    "        \"\"\"\n",
    "        hidden_state = hidden_state[first_letters]\n",
    "        hidden_state=self.init_hidden(hidden_state).unsqueeze(0)\n",
    "        return  sorted_names, sorted_names_lens, hidden_state\n",
    "\n",
    "    def forward_step(self, current_letter,hidden_state):\n",
    "        #current_letter的维度是(batch,100)\n",
    "        current_letter=current_letter.unsqueeze(0)      #让他的维度变成(1,batch,100)\n",
    "        preds, hidden_state = self.rnn(current_letter, hidden_state)  #输出就是(1,batch,100),\n",
    "        preds=self.fc(self.dropout(preds.squeeze(0)))       #变成(batch,56)\n",
    "        return preds,hidden_state\n",
    "    \n",
    "    def forward(self,names,names_len):    #names.shape=(batch,max_len+2)\n",
    "        sorted_names, sorted_names_lens, hidden_state=self.init_hidden_state(names,names_len)\n",
    "        batch_size=sorted_names.size(0)\n",
    "        lengths = sorted_names_lens.cpu().numpy() - 1\n",
    "        input=self.embed_layer(sorted_names)    #sorted_names的shape=(batch,17)->input.shape=(batch,17,100)\n",
    "        predictions = torch.zeros(batch_size, lengths[0], len(self.letter2num)).to(sorted_names.device)\n",
    "        for step in range(lengths[0]):\n",
    "            #（3）解码\n",
    "            #（3.1）模拟pack_padded_sequence函数的原理，获取该时刻的非<pad>输入\n",
    "            real_batch_size = np.where(lengths>step)[0].shape[0]\n",
    "            preds, hidden_state = self.forward_step(\n",
    "                            input[:real_batch_size, step, :],\n",
    "                            hidden_state[:, :real_batch_size, :].contiguous())            \n",
    "            # 记录结果\n",
    "            predictions[:real_batch_size, step, :] = preds\n",
    "\n",
    "        return predictions,sorted_names,lengths\n",
    "    \n",
    "\n",
    "    #给隐状态加噪声\n",
    "    def add_noise2hidden(self,hidden_state,rand_ratio,n):\n",
    "        noise=torch.randn(1,1,100)/(rand_ratio**n)\n",
    "        return hidden_state+noise\n",
    "    \n",
    "    #随机选择得分前五且差距第一不大的\n",
    "    def random_letter(self,score,indice):\n",
    "        options_list=[]\n",
    "        largest_score=score[0]\n",
    "        softmax_score=torch.softmax(score,dim=0)\n",
    "        for i in range(len(score)):\n",
    "            if(softmax_score[0]-softmax_score[i]<0.1):\n",
    "                options_list.append(indice[i])\n",
    "        return random.choice(options_list)\n",
    "\n",
    "    #预测,随机性最后在2到6之间\n",
    "    def predict(self,input_str,rand_ratio:float=3.0):\n",
    "        output=[]\n",
    "        #ell  -->ellen\n",
    "        encode_str=[self.letter2num['start']]+[self.letter2num[letter] for letter in input_str]     #[start,..,..,..]\n",
    "        length=torch.tensor([len(encode_str)])  #[4]\n",
    "        input=torch.tensor(encode_str).unsqueeze(0)     #shape=(1,4)\n",
    "        input, _, hidden_state=self.init_hidden_state(input,length)     #初始化隐状态\n",
    "        count=0     #用来计数，产生了多少个字母\n",
    "        embed_input=self.embed_layer(input)     #shape=(1,4,100)\n",
    "        while(count<length[0]):\n",
    "            if(count==length[0]-1):             #如果是最后一个已知输入产生的输出，即下一轮将使用此次输出作为输入的时候\n",
    "                hidden_state = self.add_noise2hidden(hidden_state,rand_ratio,count)\n",
    "            preds, hidden_state = self.forward_step(embed_input[:,count,:],hidden_state.contiguous())\n",
    "            #preds的shape=(1,56))\n",
    "            output.append(self.random_letter(preds[0].topk(5)[0],preds[0].topk(5)[1]))\n",
    "            count = count+1\n",
    "            if(count<length[0]):\n",
    "                if(output[-1]==encode_str[count]):     #topk(1)[1]表示第一大的元素的索引\n",
    "                    #print('第%d预测正确'%count)\n",
    "                    pass\n",
    "                else:\n",
    "                    output[-1]=encode_str[count]\n",
    "        while(output[-1]!=self.letter2num['end']):\n",
    "            #print('加入噪声')\n",
    "            hidden_state=self.add_noise2hidden(hidden_state,rand_ratio,count)\n",
    "\n",
    "            generated_letter=torch.tensor([output[-1]]).unsqueeze(0)  #shape=(1,1)\n",
    "            embed_letter=self.embed_layer(generated_letter)     #shape=(1,1,100)\n",
    "            preds, hidden_state = self.forward_step(embed_letter[0],hidden_state.contiguous())\n",
    "            \n",
    "            p=self.random_letter(preds[0].topk(5)[0],preds[0].topk(5)[1])\n",
    "\n",
    "            output.append(p)\n",
    "            count=count+1\n",
    "        #生成结束，丢掉结束符\n",
    "        output=output[:-1]\n",
    "        #将数字翻译成字母\n",
    "        #print(output)\n",
    "        output=[list(self.letter2num.keys())[list(self.letter2num.values()).index(n)] for n in output]\n",
    "        name='可以起名为：'+''.join(output)\n",
    "        return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "#构建损失函数\n",
    "class PackedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PackedCrossEntropyLoss, self).__init__()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, predictions, targets, lengths):\n",
    "        \"\"\"\n",
    "        参数：\n",
    "            predictions：按长度排序过预测结果   (batch,17,56)\n",
    "            targets：按长度排序过的真实值       (batch,17)\n",
    "            lengths：长度\n",
    "        \"\"\"\n",
    "        #predictions经过pack以后会变成(real_batch,56),real_batch取决于lengths\n",
    "        #targets经过pack以后会变成(real_batch,1),对应prediction56维匹配一个字母编号\n",
    "        predictions = pack_padded_sequence(predictions, lengths, batch_first=True)[0]\n",
    "        targets = pack_padded_sequence(targets, lengths, batch_first=True)[0]\n",
    "        return self.loss_fn(predictions, targets) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了损失函数以后，可以实例化一个网络，直接训练\n",
    "将数据丢进网络，将输出和真实值给损失函数，反传，选择优化器优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=GeneratorName(letter2num)\n",
    "model.load_state_dict(torch.load('./male_name_generator_better.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练代码块\n",
    "\n",
    "首先需要一个数据集类，继承自Dataset，改一下getitem函数，可以将传入的名字列表返回等长张量形式\n",
    "\n",
    "然后就可以把这个数据集类加载到DataLoader中\n",
    "\n",
    "再声明损失函数，使用交叉熵，但不是直接使用，而是对预测和真实值做了pack_pad_sequence操作\n",
    "\n",
    "定义一个train函数，接受要train的model，和训练数据得的地址。可以设置相关训练参数，如epoch，batch size\n",
    "\n",
    "训练的流程是，先声明出网络的实例，此时除了一些基本属性，什么都没有，在train函数中要将训练数据加载进DataLoader中，并从本地获取\n",
    "每个单词对应数字编码的字典。\n",
    "\n",
    "！！注意：一个编码字典对应一个网络参数，如果编码字典变了，需要重新训练！！！\n",
    "\n",
    "有了字典就丢给网络，网络有了字典才能构建神经网络层，是一些nn.Module\n",
    "\n",
    "然后就是for循环，开训\n",
    "\n",
    "训练完后将网络参数保存到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, step 100: loss=2.06\n",
      "epoch 100, step 100: loss=1.88\n",
      "epoch 150, step 100: loss=1.71\n",
      "epoch 200, step 100: loss=1.65\n",
      "epoch 250, step 100: loss=1.56\n",
      "epoch 300, step 100: loss=1.40\n",
      "epoch 350, step 100: loss=1.38\n",
      "epoch 400, step 100: loss=1.33\n",
      "epoch 450, step 100: loss=1.26\n",
      "epoch 500, step 100: loss=1.19\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "class NamesDataset(Dataset):\n",
    "    def __init__(self,Names:list,max_len=18):   #max_len要求每个名字最多10个字母，不足10个字母在后面补pad，也就是0\n",
    "        self.Names=Names\n",
    "        #self.transform=transform\n",
    "        self.length=len(Names)\n",
    "        self.max_len=max_len\n",
    "    def __getitem__(self, index):\n",
    "        name_len = len(self.Names[index])\n",
    "        if(name_len>self.max_len):\n",
    "            print('名字长度超过%d，自动取前15个字母'%self.max_len)\n",
    "            self.Names[index]=self.Names[index][:self.max_len]\n",
    "            name_len=self.max_len\n",
    "        return torch.LongTensor(self.Names[index]+[0]*(self.max_len+2-name_len)),name_len    #加2是考虑有起始和终止符\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "#构建损失函数\n",
    "class PackedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PackedCrossEntropyLoss, self).__init__()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, predictions, targets, lengths):\n",
    "        \"\"\"\n",
    "        参数：\n",
    "            predictions：按长度排序过预测结果   (batch,17,56)\n",
    "            targets：按长度排序过的真实值       (batch,17)\n",
    "            lengths：长度\n",
    "        \"\"\"\n",
    "        #predictions经过pack以后会变成(real_batch,56),real_batch取决于lengths\n",
    "        #targets经过pack以后会变成(real_batch,1),对应prediction56维匹配一个字母编号\n",
    "        predictions = pack_padded_sequence(predictions, lengths, batch_first=True)[0]\n",
    "        targets = pack_padded_sequence(targets, lengths, batch_first=True)[0]\n",
    "        return self.loss_fn(predictions, targets) \n",
    "\n",
    "def train_model(model:GeneratorName,data_path:str,epoch=100,lr=0.0001,batch_size=20):\n",
    "    with open(data_path) as  f:\n",
    "        #读取txt的所有行到列表中\n",
    "        name_list = f.readlines()\n",
    "        #去掉列表每一项的换行符\n",
    "    name_list = [name.strip() for name in name_list]\n",
    "\n",
    "    with open(data_path[:data_path.find('.txt')]+'_letter_encode.json','r') as f:\n",
    "        letter2num=json.load(f)     #letter2num是一个字典{\"A\":1,,...}\n",
    "    \n",
    "    model.load_letter2num(letter2num)\n",
    "    model.build_layers()\n",
    "\n",
    "    encode_names=[]\n",
    "    for i,name in enumerate(name_list):\n",
    "        encode_name = [letter2num['start']] + [letter2num[c] for c in name] + [letter2num['end']]\n",
    "        encode_names.append(encode_name)\n",
    "    \n",
    "    train_data = NamesDataset(encode_names)\n",
    "    train_loader = DataLoader(train_data,batch_size=batch_size, shuffle=True,pin_memory=True)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    loss_fn=PackedCrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    model.train()\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        for i,(names,names_len) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            names=names.to(device)\n",
    "            names_len=names_len.to(device)\n",
    "            predictions,sorted_names,lengths=model(names,names_len)\n",
    "            loss=loss_fn(predictions,sorted_names[:,1:],lengths)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if(i%100==99 and e%10==9):\n",
    "                print('epoch %d, step %d: loss=%.2f' % (e+1,i+1, loss.cpu()))\n",
    "    \n",
    "    #torch.save(model.state_dict(),data_path[:data_path.find('.txt')]+'_name_generator.pt')\n",
    "female_model=GeneratorName()\n",
    "train_model(female_model,'./female.txt',epoch=100,batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测模块\n",
    "\n",
    "包装了一个predict函数，输入名字、性别和随机量就可以预测出一个名字\n",
    "\n",
    "预测的网络是直接加载json字典和pt网络参数的，再次强调！！！字典和网络必须是一体的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'可以起名为：Taffy'"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model_by_sex(sex):\n",
    "    if(sex==1):\n",
    "        with open('./male_letter_encode.json') as f:\n",
    "            letter2num=json.load(f)\n",
    "        model=GeneratorName(letter2num)\n",
    "        model.build_layers()\n",
    "        model.load_state_dict(torch.load('./male_name_generator.pt'))\n",
    "    else:\n",
    "        with open('./female_letter_encode.json') as f:\n",
    "            letter2num=json.load(f)\n",
    "        model=GeneratorName(letter2num)\n",
    "        model.build_layers()\n",
    "        model.load_state_dict(torch.load('./female_name_generator.pt'))\n",
    "    return model\n",
    "\n",
    "def predict(name_str:str or None=None,sex:int=1,random_num:float=5):\n",
    "    if(name_str is None):\n",
    "        sex=input('想起男孩名字还是女孩名字？\\n1是男孩，0是女孩，其他退出')\n",
    "        if(sex=='0'):\n",
    "            model=build_model_by_sex(0)\n",
    "        elif(sex=='1'):\n",
    "            model=build_model_by_sex(1)\n",
    "        else:\n",
    "            return\n",
    "    \n",
    "        model.to('cpu')\n",
    "        model.eval()\n",
    "        name=input('请输入前若干字母来起英文名：(多次输入可有不同结果)')\n",
    "        return model.predict(name,3.5)\n",
    "    \n",
    "    else:\n",
    "        model=build_model_by_sex(sex)\n",
    "        return model.predict(name_str,random_num)\n",
    "\n",
    "predict('Taff',0,3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1000, 0.4000, 0.5000, 0.6000],\n",
      "        [0.5000, 0.4000, 0.3000, 0.1000],\n",
      "        [0.1000, 0.2000, 0.4000, 0.6000],\n",
      "        [0.8000, 0.8000, 0.8000, 0.6000],\n",
      "        [0.7000, 0.8000, 0.1000, 0.1000],\n",
      "        [0.9000, 0.0000, 0.0000, 0.0000]]) tensor([1, 3, 2, 2, 3, 2])\n",
      "tensor(1.5260)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "pre=torch.tensor([[[0.1,0.4,0.5,0.6],[0.1,0.2,0.4,0.6],[0.7,0.8,0.1,0.1],[0.9,0,0,0]],[[0.5,0.4,0.3,0.1],[0.8,0.8,0.8,0.6],[0,0,0,0],[0,0,0,0]]])\n",
    "targer=torch.tensor([[1,2,3,2],[3,2,0,0]])\n",
    "len=[4,2]\n",
    "pre=pack_padded_sequence(pre,len,batch_first=True)[0]\n",
    "targer=pack_padded_sequence(targer,len,batch_first=True)[0]\n",
    "print(pre,targer)\n",
    "loss_fuc=nn.CrossEntropyLoss()\n",
    "loss=loss_fuc(pre,targer)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11月13日的更新：\n",
    "\n",
    "目前仍在训练阶段，训练的效果是一个名字输入网络后，可以得到输出，输出的后几位和原输入对得上 \n",
    "说明最后一个字母能预测对\n",
    "但是前面几个字母很难对，原因显然，因为第一个字母的产生只是依赖输入的起始符号，隐状态又是随机初始化不具有任何信息\n",
    "\n",
    "因此我修改了隐状态生成的方法，我利用输入名字的第二字位置，也就是首字母对应的数字编码\n",
    "让其成为隐状态的one-hot编码，是56维向量\n",
    "然后经过softmax、线性、relu、线性转换成100维向量，这样理论上网络通过start起始符预测首字母时考虑了原输入真正的首字母\n",
    "\n",
    "从这个网络的功能来看，因为它是要根据给出的一个或多个字母来预测名字，在生成第一个字母，如果不考虑输入的首字母，仅仅依赖起始符和\n",
    "没有任何信息的隐状态，生成首字母是完全没有依据的，如果不是输入的首字母，很有可能会极大影响后续字母生成\n",
    "就算生成对应的首字母，也可能不是大写的。第一个起始字母只要预测准确，就算是给后面的预测提供了方向\n",
    "\n",
    "从训练的结果来看，在没有加这个方法时，在500个epoch以上loss也是在1.4到1.6中徘徊，使用这个方法后，300个epoch就出现了1.2的loss，\n",
    "在约1000epoch的训练后，观察其中10个epoch的每个的100的step的loss，肉眼估计在1.2左右，更有两次1.07和1.08\n",
    "\n",
    "随便将两个名字丢进网络中，产生的结果是：首字母能够准确预测，最后3个以上(包含结束符)字母也能准确预测，一般错误的是第二第三个，可以理解，\n",
    "哪怕让人来起名字，只知道第一个字母是A，也不可能完全预测是Ann，Alb，Aus等，但后面几个能预测对，恰恰说明网络具有记忆能力，能够根据前文和当前输入预测下一个字母"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11月14日更新\n",
    "\n",
    "目前网络已经完成了预测功能，输入前几个字母能返回一个非常正常的名字，\n",
    "但是这个网络没有随机性，输入Al他永远的发挥的都是Alex，这样不够好\n",
    "\n",
    "因此我选择给他添加随机性，从两个方面入手。\n",
    "\n",
    "第一：返回的名字中，与输入无关，即网络预测的字母，生成他们的时候是通过上一个字母和上一个隐状态丢给GRU实现的，给隐状态加入随机噪声，越后面的随机噪声越弱，在前期尽可能能有更多样的分叉路。\n",
    "\n",
    "第二：每次生成字母时，不再直接选得分最大的，而是先选择最大的5个字母，再根据前3个字母对于5个得分平均值的偏移程度，足够则加入到随机数组中，通过一个随机数取得这个数组的其中一个作为输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11月15日更新\n",
    "\n",
    "网络已经完成，并且完善了一些小细节，例如训练的名字如果太长，会直接舍弃过长的部分\n",
    "\n",
    "并且将训练和预测两个模块封装了起来"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
