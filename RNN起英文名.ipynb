{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "32097627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aamir', 'Aaron', 'Abbey', 'Abbie', 'Abbot']\n"
     ]
    }
   ],
   "source": [
    "with open('./male.txt') as  f:\n",
    "    #读取txt的所有行到列表中\n",
    "    male_name_list = f.readlines()\n",
    "    #去掉列表每一项的换行符\n",
    "    male_name_list = [name.strip() for name in male_name_list]\n",
    "print(male_name_list[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对数据做处理：\n",
    "1.从txt文件中读入所有名字，并以列表存储，列表的每个元素都是一个字符串\n",
    "2.每个字符串都由字母letter组成，统计所有出现过的字母\n",
    "3.在本任务中，RNN网络每一次输入应该是一个字母，因此要对字母编码\n",
    "4.编码的方式非常简单，按照顺序给每个字母配一个int,还要再加上起始符start、终止符end\n",
    "5.将所有字符串形式的名字全部编码成数组形式，并在每个名字加上起始和结束标识的编码，例如“Adam”->[0,1,10,2,3,54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[54, 1, 2, 5, 6, 7, 55], [54, 1, 8, 8, 9, 10, 55]]\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import json\n",
    "letter = Counter()\n",
    "for name in male_name_list:\n",
    "    for char in name:\n",
    "        letter.update(char)\n",
    "'''\n",
    "letter2num = {k:i+1 for i , k in enumerate(letter)}\n",
    "letter2num['pad'] = 0\n",
    "letter2num['start']=len(letter2num)\n",
    "letter2num['end']=len(letter2num)\n",
    "num2letter = {letter2num[k]:k for k in letter2num.keys()}\n",
    "print(letter2num)\n",
    "'''\n",
    "with open('./letter2num_encode.json','r') as f:\n",
    "    letter2num=json.load(f)\n",
    "encode_names=[]\n",
    "names_len = []\n",
    "for i,name in enumerate(male_name_list):\n",
    "    encode_name = [letter2num['start']] + [letter2num[c] for c in name] + [letter2num['end']]\n",
    "    encode_names.append(encode_name)\n",
    "    names_len.append(len(encode_name))\n",
    "print(encode_names[1:3])\n",
    "print(names_len.index(min(names_len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在网络中，每个名字要是一维张量，shape=(17,)，第一个元素一定是54，表示开始，中间会有55，表示结束，55后面全是0，用来占位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch\n",
    "class NamesDataset(Dataset):\n",
    "    def __init__(self,Names:list,max_len=15):   #max_len要求每个名字最多10个字母，不足10个字母在后面补pad，也就是0\n",
    "        self.Names=Names\n",
    "        #self.transform=transform\n",
    "        self.length=len(Names)\n",
    "        self.max_len=max_len\n",
    "    def __getitem__(self, index):\n",
    "        name_len = len(self.Names[index])\n",
    "        return torch.LongTensor(self.Names[index]+[0]*(self.max_len+2-name_len)),name_len    #加2是考虑有起始和终止符\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "train_data = NamesDataset(encode_names)\n",
    "train_loader = DataLoader(train_data,batch_size=20, shuffle=True,pin_memory=True)\n",
    "#print:tensor([[54, 47, 18, 21,  6, 55,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "        #      [54, 50,  8, 14, 10,  2, 12, 55,  0,  0,  0,  0,  0,  0,  0,  0,  0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64 tensor([11, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "for i , (names,lens) in enumerate(train_loader):\n",
    "    print(names[:3].dtype,lens[:3])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN的输入虽然说是一个序列，但其实每次真正拿进去网络和隐层h一起搞的只有一个token，在这里就是一个字母\n",
    "目前的一个名字的表示已经转换为了一个shape=(12,)的一维张量，但每个字母只是一个int而已，这样当然不行\n",
    "需要把int转换成tensor\n",
    "考虑所有的字母，包括起始符、终止符和pad，一共有56个字母，即len(letter2num)=56\n",
    "可以使用nn.embedding(56,100),让56个字母，每个都用一个shape=(100,)的一维张量表示\n",
    "原本的names=(batch,max_len+2)经过embed作为网络的input=(batch,max_len+2,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class GeneratorName(nn.Module):\n",
    "    def __init__(self,letter2num,letter_dim,dropout=0.3,hidden_size=100) -> None:\n",
    "        super(GeneratorName,self).__init__()\n",
    "        self.letter_count=len(letter2num)\n",
    "        self.hidden_size=hidden_size\n",
    "        self.letter2num=letter2num\n",
    "        self.letter_dim=letter_dim\n",
    "        #hidden最开始是56维的one-hot向量，将其经过线性层\n",
    "        self.init_hidden=nn.Sequential(\n",
    "            nn.Softmax(dim=1),\n",
    "            nn.Linear(self.letter_count,hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size,hidden_size),\n",
    "        )\n",
    "        self.embed_layer=nn.Embedding(self.letter_count,letter_dim)\n",
    "        self.rnn=nn.GRU(letter_dim,self.hidden_size,1)      #rnn的输入是100，因为每个字母转换成了100维向量,输出是100维向量\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.fc=nn.Linear(self.hidden_size,self.letter_count)   #用线性层将100维变成56维\n",
    "\n",
    "    def init_hidden_state(self, names, names_len):\n",
    "        #names=(batch,max_len+2),names_len=(batch,)\n",
    "        batch_size, names_dim = names.size(0), names.size(1)\n",
    "\n",
    "        # 按照names的长短排序\n",
    "        sorted_names_lens, sorted_nams_indices = torch.sort(names_len, 0, True)\n",
    "        sorted_names = names[sorted_nams_indices]\n",
    "\n",
    "        #hidden_state = torch.zeros(batch_size,self.letter_count).to(sorted_names.device)    #隐层状态的shape=(1,batch,100)\n",
    "        hidden_state = torch.eye(self.letter_count).to(sorted_names.device)\n",
    "        first_letters=sorted_names[:,1]\n",
    "        \"\"\"\n",
    "        for i in range(batch_size):\n",
    "            hidden_state[i , first_letters[i]] = 1\n",
    "        \"\"\"\n",
    "        hidden_state = hidden_state[first_letters]\n",
    "        hidden_state=self.init_hidden(hidden_state).unsqueeze(0)\n",
    "        return  sorted_names, sorted_names_lens, hidden_state\n",
    "\n",
    "    def forward_step(self, current_letter,hidden_state):\n",
    "        #current_letter的维度是(batch,100)\n",
    "        current_letter=current_letter.unsqueeze(0)      #让他的维度变成(1,batch,100)\n",
    "        preds, hidden_state = self.rnn(current_letter, hidden_state)  #输出就是(1,batch,100),\n",
    "        preds=self.fc(self.dropout(preds.squeeze(0)))       #变成(batch,56)\n",
    "        return preds,hidden_state\n",
    "    \n",
    "    def forward(self,names,names_len):    #names.shape=(batch,max_len+2)\n",
    "        sorted_names, sorted_names_lens, hidden_state=self.init_hidden_state(names,names_len)\n",
    "        batch_size=sorted_names.size(0)\n",
    "        lengths = sorted_names_lens.cpu().numpy() - 1\n",
    "        input=self.embed_layer(sorted_names)    #sorted_names的shape=(batch,17)->input.shape=(batch,17,100)\n",
    "        predictions = torch.zeros(batch_size, lengths[0], len(self.letter2num)).to(sorted_names.device)\n",
    "        for step in range(lengths[0]):\n",
    "            #（3）解码\n",
    "            #（3.1）模拟pack_padded_sequence函数的原理，获取该时刻的非<pad>输入\n",
    "            real_batch_size = np.where(lengths>step)[0].shape[0]\n",
    "            preds, hidden_state = self.forward_step(\n",
    "                            input[:real_batch_size, step, :],\n",
    "                            hidden_state[:, :real_batch_size, :].contiguous())            \n",
    "            # 记录结果\n",
    "            predictions[:real_batch_size, step, :] = preds\n",
    "\n",
    "        return predictions,sorted_names,lengths\n",
    "    \n",
    "    def add_noise2hidden(self,hidden_state,rand_ratio,n):\n",
    "        noise=torch.randn(1,1,100)/(rand_ratio**n)\n",
    "        return hidden_state+noise\n",
    "\n",
    "    def predict(self,input_str,rand_ratio:int=5):\n",
    "        output=[]\n",
    "        #ell  -->ellen\n",
    "        encode_str=[self.letter2num['start']]+[self.letter2num[letter] for letter in input_str]     #[start,..,..,..]\n",
    "        length=torch.tensor([len(encode_str)])  #[4]\n",
    "        input=torch.tensor(encode_str).unsqueeze(0)     #shape=(1,4)\n",
    "        input, _, hidden_state=self.init_hidden_state(input,length)     #初始化隐状态\n",
    "        count=0     #用来计数，产生了多少个字母\n",
    "        embed_input=self.embed_layer(input)     #shape=(1,4,100)\n",
    "        while(count<length[0]):\n",
    "            if(count==length[0]-1):             #如果是最后一个已知输入产生的输出，即下一轮将使用此次输出作为输入的时候\n",
    "                print('加入噪声')\n",
    "                #hidden_state = self.add_noise2hidden(hidden_state,rand_ratio,count)\n",
    "                #hidden_state=hidden_state+(torch.rand(1,1,100)-1)/(rand_ratio**count)  幂指数\n",
    "                #hidden_state=hidden_state+(torch.rand(1,1,100))/(rand_ratio*count)     线性\n",
    "            preds, hidden_state = self.forward_step(embed_input[:,count,:],hidden_state.contiguous())\n",
    "            #preds的shape=(1,56)\n",
    "            print(preds[0].topk(5)[0])\n",
    "            output.append(preds[0].topk(1)[1].item())\n",
    "            count = count+1\n",
    "            if(count<length[0]):\n",
    "                if(preds[0].topk(1)[1].item()==encode_str[count]):     #topk(1)[1]表示第一大的元素的索引\n",
    "                    #print('第%d预测正确'%count)\n",
    "                    pass\n",
    "                else:\n",
    "                    output[-1]=encode_str[count]\n",
    "        while(output[-1]!=self.letter2num['end']):\n",
    "            print('加入噪声')\n",
    "            hidden_state=hidden_state+(torch.rand(1,1,100))/(rand_ratio*count)\n",
    "\n",
    "            generated_letter=torch.tensor([output[-1]]).unsqueeze(0)  #shape=(1,1)\n",
    "            embed_letter=self.embed_layer(generated_letter)     #shape=(1,1,100)\n",
    "            preds, hidden_state = self.forward_step(embed_letter[0],hidden_state.contiguous())\n",
    "            print(preds[0].topk(5)[0])\n",
    "            output.append(preds[0].topk(1)[1].item())\n",
    "            count=count+1\n",
    "        #生成结束，丢掉结束符\n",
    "        output=output[:-1]\n",
    "        #将数字翻译成字母\n",
    "        #print(output)\n",
    "        output=[list(self.letter2num.keys())[list(self.letter2num.values()).index(n)] for n in output]\n",
    "        name='可以起名为：'+''.join(output)\n",
    "        return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "#构建损失函数\n",
    "class PackedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PackedCrossEntropyLoss, self).__init__()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, predictions, targets, lengths):\n",
    "        \"\"\"\n",
    "        参数：\n",
    "            predictions：按长度排序过预测结果   (batch,17,56)\n",
    "            targets：按长度排序过的真实值       (batch,17)\n",
    "            lengths：长度\n",
    "        \"\"\"\n",
    "        #predictions经过pack以后会变成(real_batch,56),real_batch取决于lengths\n",
    "        #targets经过pack以后会变成(real_batch,1),对应prediction56维匹配一个字母编号\n",
    "        predictions = pack_padded_sequence(predictions, lengths, batch_first=True)[0]\n",
    "        targets = pack_padded_sequence(targets, lengths, batch_first=True)[0]\n",
    "        return self.loss_fn(predictions, targets) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了损失函数以后，可以实例化一个网络，直接训练\n",
    "将数据丢进网络，将输出和真实值给损失函数，反传，选择优化器优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 709,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=GeneratorName(letter2num,letter_dim=100)\n",
    "model.load_state_dict(torch.load('./male_name_generator_better.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 100: loss=0.97\n",
      "tensor([43, 13, 11,  2, 26,  4,  6, 18, 55], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "loss_fn=PackedCrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.0001)\n",
    "model.train()\n",
    "epoch=1\n",
    "for e in range(epoch):\n",
    "    for i,(names,names_len) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        names=names.to(device)\n",
    "        names_len=names_len.to(device)\n",
    "        predictions,sorted_names,lengths=model(names,names_len)\n",
    "        loss=loss_fn(predictions,sorted_names[:,1:],lengths)\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if(i%100==99 and e%50==0):\n",
    "            print('epoch %d, step %d: loss=%.2f' % (e+1,i+1, loss.cpu()))\n",
    "test=torch.tensor([[54,  1, 14,  8,  9,  5, 11, 55,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "                    [54, 43, 17, 11,  2, 26,  4, 14, 18, 55,  0,  0,  0,  0,  0,  0,  0]]).to(device)\n",
    "l=torch.tensor([8,10]).to(device)\n",
    "output,_,l=model(test,l)\n",
    "print(output[0,:].topk(1)[1].squeeze(1))\n",
    "#print(output[1,:].topk(1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([19.1680, 10.5689, 10.0685,  9.7947,  9.0448], grad_fn=<TopkBackward0>)\n",
      "加入噪声\n",
      "tensor([9.0989, 7.8994, 7.8681, 7.7445, 6.6455], grad_fn=<TopkBackward0>)\n",
      "加入噪声\n",
      "tensor([6.1983, 4.9830, 4.3157, 4.2806, 3.8948], grad_fn=<TopkBackward0>)\n",
      "加入噪声\n",
      "tensor([5.0666, 4.7217, 4.6972, 4.4640, 4.1620], grad_fn=<TopkBackward0>)\n",
      "加入噪声\n",
      "tensor([6.8447, 6.6583, 6.1654, 5.5612, 5.0130], grad_fn=<TopkBackward0>)\n",
      "加入噪声\n",
      "tensor([6.2451, 5.7855, 5.5841, 4.2721, 3.8921], grad_fn=<TopkBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'可以起名为：Marco'"
      ]
     },
     "execution_count": 710,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "model.eval()\n",
    "#name=input('请输入前若干字母来起英文名：')\n",
    "#model.predict(name)\n",
    "model.predict('M',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'./male_name_generator_better.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1000, 0.4000, 0.5000, 0.6000],\n",
      "        [0.5000, 0.4000, 0.3000, 0.1000],\n",
      "        [0.1000, 0.2000, 0.4000, 0.6000],\n",
      "        [0.8000, 0.8000, 0.8000, 0.6000],\n",
      "        [0.7000, 0.8000, 0.1000, 0.1000],\n",
      "        [0.9000, 0.0000, 0.0000, 0.0000]]) tensor([1, 3, 2, 2, 3, 2])\n",
      "tensor(1.5260)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "pre=torch.tensor([[[0.1,0.4,0.5,0.6],[0.1,0.2,0.4,0.6],[0.7,0.8,0.1,0.1],[0.9,0,0,0]],[[0.5,0.4,0.3,0.1],[0.8,0.8,0.8,0.6],[0,0,0,0],[0,0,0,0]]])\n",
    "targer=torch.tensor([[1,2,3,2],[3,2,0,0]])\n",
    "len=[4,2]\n",
    "pre=pack_padded_sequence(pre,len,batch_first=True)[0]\n",
    "targer=pack_padded_sequence(targer,len,batch_first=True)[0]\n",
    "print(pre,targer)\n",
    "loss_fuc=nn.CrossEntropyLoss()\n",
    "loss=loss_fuc(pre,targer)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11月13日的更新：\n",
    "目前仍在训练阶段，训练的效果是一个名字输入网络后，可以得到输出，输出的后几位和原输入对得上 \n",
    "说明最后一个字母能预测对\n",
    "但是前面几个字母很难对，原因显然，因为第一个字母的产生只是依赖输入的起始符号，隐状态又是随机初始化不具有任何信息\n",
    "\n",
    "因此我修改了隐状态生成的方法，我利用输入名字的第二字位置，也就是首字母对应的数字编码\n",
    "让其成为隐状态的one-hot编码，是56维向量\n",
    "然后经过softmax、线性、relu、线性转换成100维向量，这样理论上网络通过start起始符预测首字母时考虑了原输入真正的首字母\n",
    "\n",
    "从这个网络的功能来看，因为它是要根据给出的一个或多个字母来预测名字，在生成第一个字母，如果不考虑输入的首字母，仅仅依赖起始符和\n",
    "没有任何信息的隐状态，生成首字母是完全没有依据的，如果不是输入的首字母，很有可能会极大影响后续字母生成\n",
    "就算生成对应的首字母，也可能不是大写的。第一个起始字母只要预测准确，就算是给后面的预测提供了方向\n",
    "\n",
    "从训练的结果来看，在没有加这个方法时，在500个epoch以上loss也是在1.4到1.6中徘徊，使用这个方法后，300个epoch就出现了1.2的loss，\n",
    "在约1000epoch的训练后，观察其中10个epoch的每个的100的step的loss，肉眼估计在1.2左右，更有两次1.07和1.08\n",
    "\n",
    "随便将两个名字丢进网络中，产生的结果是：首字母能够准确预测，最后3个以上(包含结束符)字母也能准确预测，一般错误的是第二第三个，可以理解，\n",
    "哪怕让人来起名字，只知道第一个字母是A，也不可能完全预测是Ann，Alb，Aus等，但后面几个能预测对，恰恰说明网络具有记忆能力，能够根据前文和当前输入预测下一个字母"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
